{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%%bash\n",
    "pip -q install trl\n",
    "pip -q install peft\n",
    "pip -q install torch\n",
    "pip -q install datasets\n",
    "pip -q install transformers\n",
    "pip -q install peft"
   ],
   "id": "7dffe95f43ede62c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:12:07.858509Z",
     "start_time": "2024-09-28T17:12:07.846891Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from trl import SFTTrainer\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments\n",
    "import pandas as pd"
   ],
   "id": "12368d800fd97a1c",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:19:41.775118Z",
     "start_time": "2024-09-28T17:19:41.223024Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# df = pd.read_csv('TRAIN_DATA.csv')\n",
    "# df\n",
    "\n",
    "# data_files = {\"train\":\"TRAIN_DATA.parquet\",\"test\":\"TEST_DATA.parquet\"}\n",
    "# raw_datasets = load_dataset(\"parquet\", data_dir='./', data_files=data_files)\n",
    "# raw_datasets\n",
    "\n",
    "# train_dataset = load_from_disk('./TRAIN_DATA.parquet')\n",
    "\n",
    "# from datasets import load_dataset\n",
    "dataset = load_dataset('csv', data_files='TRAIN_DATA.csv')\n",
    "dataset_A = dataset['train']\n",
    "\n",
    "# dataset = load_dataset('csv', data_files={'train': 'TRAIN_DATA.csv', 'test': 'TRAIN_DATA.csv'})\n",
    "dataset_A"
   ],
   "id": "ccefc2c6c19fc04a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input', 'instruction', 'output', 'text'],\n",
       "    num_rows: 17500\n",
       "})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:49:41.394063Z",
     "start_time": "2024-09-28T16:49:41.391078Z"
    }
   },
   "cell_type": "code",
   "source": "import textwrap",
   "id": "a77078a7a17f2012",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:49:41.759885Z",
     "start_time": "2024-09-28T16:49:41.754641Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for index in range(3):\n",
    "    print(\"---\"*15)\n",
    "    print(\"Instruction: {}\".format(textwrap.fill(df.iloc[index][\"instruction\"],width=50)))\n",
    "    print(\"Output: {}\".format(textwrap.fill(df.iloc[index][\"output\"],width=50)))\n",
    "    print(\"Text: {}\".format(textwrap.fill(df.iloc[index][\"text\"],width=50)))"
   ],
   "id": "8ab489b98f592227",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------\n",
      "Instruction: Reduce stress. Find a way to manage the workload\n",
      "without constant burnout.\n",
      "Output: Delegate tasks to colleagues where possible.\n",
      "Prioritize tasks based on urgency and importance.\n",
      "Speak to a manager about workload concerns. Break\n",
      "large tasks into smaller, manageable parts.\n",
      "Implement time management techniques, such as the\n",
      "Pomodoro technique.\n",
      "Text: Delegating tasks may lighten the load but depends\n",
      "on team dynamics. Prioritizing tasks will likely\n",
      "be effective as it focuses on key deliverables.\n",
      "Speaking to the manager might result in either\n",
      "assistance or further stress, depending on the\n",
      "relationship. Breaking tasks down will help\n",
      "maintain productivity without feeling overwhelmed.\n",
      "Time management techniques are easy to implement\n",
      "and could help immediately.\n",
      "---------------------------------------------\n",
      "Instruction: Reduce the frequency of arguments. Improve\n",
      "communication with the partner. Foster a more\n",
      "positive and supportive relationship.\n",
      "Output: Set up weekly times for calm discussions where\n",
      "both can express their feelings. Attend couples\n",
      "counseling. Learn and practice active listening\n",
      "techniques. Take a break or cool-down period\n",
      "during heated arguments to avoid escalation.\n",
      "Text: Scheduled conversations can provide a neutral,\n",
      "calm setting for discussions. Couples counseling\n",
      "may provide long-term benefits but requires\n",
      "commitment from both. Active listening can\n",
      "immediately improve understanding and reduce\n",
      "miscommunication. Cooling down during arguments\n",
      "can help defuse tension and prevent hurtful\n",
      "comments.\n",
      "---------------------------------------------\n",
      "Instruction: Reduce the anxiety and manage fears surrounding\n",
      "the procedure. Gain a clearer understanding of\n",
      "what the procedure entails.\n",
      "Output: Research the procedure to better understand its\n",
      "risks and benefits. Schedule a meeting with the\n",
      "doctor to discuss concerns. Practice relaxation\n",
      "techniques (deep breathing, meditation) to manage\n",
      "immediate anxiety. Seek support from a friend or\n",
      "family member who has undergone a similar\n",
      "procedure.\n",
      "Text: Researching the procedure can provide reassurance\n",
      "but may increase anxiety if done excessively.\n",
      "Talking to the doctor will offer personalized and\n",
      "professional advice. Relaxation techniques can be\n",
      "immediately helpful in reducing physical symptoms\n",
      "of anxiety. Emotional support from someone with\n",
      "experience can provide comfort.\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:51:50.084628Z",
     "start_time": "2024-09-28T16:49:42.978189Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pretrained_model_name = \"Salesforce/xgen-7b-8k-base\"\n",
    "model = AutoModelForCausalLM.from_pretrained(pretrained_model_name, torch_dtype=torch.bfloat16)\n"
   ],
   "id": "4a7a60a96392fcc1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "554a6457752a49cdaf4d4adce01276ec"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:51:55.175484Z",
     "start_time": "2024-09-28T16:51:54.148069Z"
    }
   },
   "cell_type": "code",
   "source": "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name, trust_remote_code=True)",
   "id": "d1ef48253ccac2c6",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:52:32.459872Z",
     "start_time": "2024-09-28T16:52:32.393585Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_training_args = TrainingArguments(\n",
    "    output_dir=\"xgen-7b-8k-base-fine-tuned\",\n",
    "    per_device_train_batch_size=4,\n",
    "    optim=\"adamw_torch\",\n",
    "    logging_steps=80,\n",
    "    learning_rate=2e-4,\n",
    "    warmup_ratio=0.1,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    num_train_epochs=100,\n",
    "    save_strategy=\"epoch\"\n",
    ")"
   ],
   "id": "6e4dee08d75ce0d6",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T16:57:34.081327Z",
     "start_time": "2024-09-28T16:57:33.360776Z"
    }
   },
   "cell_type": "code",
   "source": "lora_peft_config = LoraConfig(r=16, lora_alpha=32, lora_dropout=0.05, task_type=\"CAUSAL_LM\")",
   "id": "57485de55ebbdccb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-09-28T17:21:51.141571Z",
     "start_time": "2024-09-28T17:19:55.680129Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SFT_trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset_A,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    args=model_training_args,\n",
    "    peft_config=lora_peft_config,\n",
    ")"
   ],
   "id": "597ecaecf4828a5f",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jan/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "/Users/jan/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:283: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "/Users/jan/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:321: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/17500 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6cd7fd55f1724edbb34dc4860d3d31b8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 9.04 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 86.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[29], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m SFT_trainer \u001B[38;5;241m=\u001B[39m \u001B[43mSFTTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      3\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdataset_A\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      4\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdataset_text_field\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mtext\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m      6\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      7\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_training_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      8\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlora_peft_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m      9\u001B[0m \u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/huggingface_hub/utils/_deprecation.py:101\u001B[0m, in \u001B[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m     99\u001B[0m         message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m custom_message\n\u001B[1;32m    100\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[0;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/trl/trainer/sft_trainer.py:401\u001B[0m, in \u001B[0;36mSFTTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001B[0m\n\u001B[1;32m    395\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mpadding_side \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mpadding_side \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m    396\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[1;32m    397\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m` to your code.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    399\u001B[0m     )\n\u001B[0;32m--> 401\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m    402\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    403\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    404\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    405\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    406\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    407\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    408\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    409\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    410\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    411\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    412\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    413\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    415\u001B[0m \u001B[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001B[39;00m\n\u001B[1;32m    416\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madd_model_tags\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/transformers/trainer.py:554\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[1;32m    549\u001B[0m \u001B[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001B[39;00m\n\u001B[1;32m    550\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m    551\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplace_model_on_device\n\u001B[1;32m    552\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_method\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mBITS_AND_BYTES\n\u001B[1;32m    553\u001B[0m ):\n\u001B[0;32m--> 554\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_move_model_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    556\u001B[0m \u001B[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001B[39;00m\n\u001B[1;32m    557\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_model_parallel:\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/transformers/trainer.py:802\u001B[0m, in \u001B[0;36mTrainer._move_model_to_device\u001B[0;34m(self, model, device)\u001B[0m\n\u001B[1;32m    801\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_move_model_to_device\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, device):\n\u001B[0;32m--> 802\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    803\u001B[0m     \u001B[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001B[39;00m\n\u001B[1;32m    804\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mparallel_mode \u001B[38;5;241m==\u001B[39m ParallelMode\u001B[38;5;241m.\u001B[39mTPU \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtie_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1174\u001B[0m, in \u001B[0;36mModule.to\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m   1171\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1172\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[0;32m-> 1174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[0;31m[... skipping similar frames: Module._apply at line 780 (4 times)]\u001B[0m\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:805\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    801\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[1;32m    802\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[1;32m    803\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[1;32m    804\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m--> 805\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    806\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[1;32m    808\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[0;32m~/DataspellProjects/ips_llama/venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1160\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[0;34m(t)\u001B[0m\n\u001B[1;32m   1153\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[1;32m   1154\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[1;32m   1155\u001B[0m             device,\n\u001B[1;32m   1156\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m   1157\u001B[0m             non_blocking,\n\u001B[1;32m   1158\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[1;32m   1159\u001B[0m         )\n\u001B[0;32m-> 1160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   1163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1164\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m   1166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 9.04 GB, other allocations: 384.00 KB, max allowed: 9.07 GB). Tried to allocate 86.00 MB on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "3e8a9440915ebbbb",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
